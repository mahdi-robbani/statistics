length(poly_lm_LL)
plot(1:9, poly_lm_AIC, col = "red")
plot(1:9, poly_lm_LL, col = "blue")
plot(1:9, poly_lm_AIC, col = "red")
plot(1:9, poly_lm_LL, col = "blue")
#Ex 2.6 Perform model selection between the simple linear regression of point 2.2 and the polynomial regression in point 2.4. Use the log-likelihood ratio test, the F-test (both with anova). Moreover perform model selection also using AIC and BIC score (AIC, BIC)
AIC(poly_lm_simple, poly_lm_advanced)
Y <- sapply(X, function(x) rnorm(1, mean= x^2 - x + 1, sd = 2))
plot(X, Y)
#Ex 2.6 Perform model selection between the simple linear regression of point 2.2 and the polynomial regression in point 2.4. Use the log-likelihood ratio test, the F-test (both with anova). Moreover perform model selection also using AIC and BIC score (AIC, BIC)
AIC(poly_lm_simple, poly_lm_advanced)
BIC(poly_lm_simple, poly_lm_advanced)
#Ex 2.7 Try to fit now a polynomial of higher degree (e.g. 3,4,5,...). Perform also here model selection. In particular plot the AIC (or BIC) score as a function of the polynomial degree. Plot also the log-likelihood as a function of the polynomial degree. What can you observe? What is the difference between the pure log-likelihood and the AIC and BIC scores?
poly_lm_list <- lapply(2:10, function(i) lm(Y ~ I(X^i) + X))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(1:9, poly_lm_AIC, col = "red")
plot(1:9, poly_lm_LL, col = "blue")
plot(1:9, poly_lm_AIC, col = "red")
poly_lm_list
poly_lm_advanced
poly_lm_list
poly_lm_simple
poly_lm_advanced
lm(Y ~ I(X^2) + X)
lapply(1, function(i) lm(Y ~ I(X^i) + X))
lapply(2, function(i) lm(Y ~ I(X^i) + X))
#Ex 2.7 Try to fit now a polynomial of higher degree (e.g. 3,4,5,...). Perform also here model selection. In particular plot the AIC (or BIC) score as a function of the polynomial degree. Plot also the log-likelihood as a function of the polynomial degree. What can you observe? What is the difference between the pure log-likelihood and the AIC and BIC scores?
poly_lm_list <-
lm(Y ~ I(X^2) + X)
#Ex 2.7 Try to fit now a polynomial of higher degree (e.g. 3,4,5,...). Perform also here model selection. In particular plot the AIC (or BIC) score as a function of the polynomial degree. Plot also the log-likelihood as a function of the polynomial degree. What can you observe? What is the difference between the pure log-likelihood and the AIC and BIC scores?
poly_lm_list <-
lm(Y ~ I(X^2) + X)
lm(Y ~ I(X^2) + X)
lapply(2, function(i) lm(Y ~ I(X^i) + X))
lapply(2:3, function(i) lm(Y ~ I(X^i) + X))
lm(Y ~ I(X^2) + X)
lm(Y ~ I(X^3) + X)
poly_lm_list <- lapply(2:10, function(i) lm(Y ~ I(X^i) + X))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(2:10, poly_lm_AIC, col = "red")
plot(2:10, poly_lm_LL, col = "blue")
poly_lm_AIC
poly_lm_AIC
poly_lm_list
lm(Y ~ I(X^2) + X)
AIC(lm(Y ~ I(X^2) + X))
AIC(lm(Y ~ I(X^3) + X))
plot(2:10, poly_lm_LL, col = "blue")
plot(2:10, poly_lm_AIC, col = "red")
plot(2:10, poly_lm_LL, col = "blue")
X <- rnorm(50, mean=0, sd = 0.25)
Y <- sapply(X, function(x) rnorm(1, mean= exp(-3*x) + 2*x, sd = 2))
#Ex 3.2 Fit a simple linear model E(Y jX = x) = beta_0 +beta_1x, and polynomial regression models up to degree 5.
exp_lm_list <- lapply(1:5, function(i) lm(Y ~ I(X^i) + X))
exp_lm_list
exp_lm_AIC <- sapply(exp_lm_list, function(x) AIC(x))
exp_lm_BIC <- sapply(exp_lm_list, function(x) BIC(x))
exp_lm_df <- data.frame("Polynomial" = 1:5, "AIC" = exp_lm_AIC, "BIC" = exp_lm_BIC)
exp_lm_BIC # x^2 gives the best model
min(exp_lm_BIC) # x^2 gives the best model
plot(1, type = "n", xlim=c(-0.7, 0.7), ylim = c(-4.5, 4.5), xlab = "X", ylab= "Y")
for(i in 1:5){
points(X, residuals(exp_lm_list[[i]]), col = i+1)
curve(x^i +2*x, add = TRUE, col = i+1)
}
legend("topleft", c("X^1", "X^2", "X^3", "X^4", "X^5"), col = c(2:6), lty = 1, cex = 0.75)
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ 2*X + exp(-3* X))
exp_lm
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ 2*X + exp(-3* X))
X <- rnorm(50, mean=0, sd = 0.25)
Y <- sapply(X, function(x) rnorm(1, mean= exp(-3*x) + 2*x, sd = 2))
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ 2*X + exp(-3* X))
Y
X
?nls
nls(Y ~ 2*X + exp(-3* X))
Comment.
plot(1, type = "n", xlim=c(-0.7, 0.7), ylim = c(-4.5, 4.5), xlab = "X", ylab= "Y")
for(i in 1:5){
points(X, residuals(exp_lm_list[[i]]), col = i+1)
curve(x^i +2*x, add = TRUE, col = i+1)
}
legend("topleft", c("X^1", "X^2", "X^3", "X^4", "X^5"), col = c(2:6), lty = 1, cex = 0.75)
#Ex
3+2
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ 2*X + exp(-3* X))
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(formula=Y ~ 2*X + exp(-3* X)
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(formula=Y ~ 2*X + exp(-3* X))
exp_lm
nls(formula=Y ~ 2*X + exp(-3* X))
nls(Y ~ 2*X + exp(-3* X))
X
df_XY <- data.frame("X" = X, "Y" = Y)
df_XY
nls(formula = Y ~ 2*X + exp(-3* X), data = df_XY)
#Ex 1.3 Use the function summary to obtain informations on the coeffcients of the model.
summary(lm_ab)
modelXY(GenXY(a = 3,b = 2)) #p value is 0.18 so we do not reject H_0
AIC(intercept, no_intercept)
BIC(intercept, no_intercept)
anova(intercept, no_intercept, test = "F")
#Ex 2.7 Try to fit now a polynomial of higher degree (e.g. 3,4,5,...). Perform also here model selection. In particular plot the AIC (or BIC) score as a function of the polynomial degree. Plot also the log-likelihood as a function of the polynomial degree. What can you observe? What is the difference between the pure log-likelihood and the AIC and BIC scores?
poly_lm_list <- lapply(2:10, function(i) lm(Y ~ I(X^i) + X))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(2:10, poly_lm_AIC, col = "red")
plot(2:10, poly_lm_LL, col = "blue")
poly_lm_list[[1]]
lm(Y ~ I(X^2) + X)
poly_lm_list[[1]]
AIC(lm(Y ~ I(X^2) + X))
AIC(lm(Y ~ I(X^3) + X))
AIC(lm(Y ~ I(X^4) + X))
X <- rnorm(50, mean = 0, sd = 1)
Y <- sapply(X, function(x) rnorm(1, mean= x^2 - x + 1, sd = 2))
poly_lm_list <- lapply(2:10, function(i) lm(Y ~ I(X^i) + X))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(2:10, poly_lm_AIC, col = "red")
plot(2:10, poly_lm_LL, col = "blue")
plot(2:10, poly_lm_AIC, col = "red")
plot(2:10, poly_lm_LL, col = "blue")
#Ex 3
#Ex 3.1 As in exercise 1 and 2 generate n = 50 observations from the following model
X <- rnorm(50, mean=0, sd = 0.25)
Y <- sapply(X, function(x) rnorm(1, mean= exp(-3*x) + 2*x, sd = 2))
#Ex 3.2 Fit a simple linear model E(Y jX = x) = beta_0 +beta_1x, and polynomial regression models up to degree 5.
exp_lm_list <- lapply(1:5, function(i) lm(Y ~ I(X^i) + X))
exp_lm_list[[1]]
lm(Y ~ I(X^1) + X)
exp_lm_list[[2]]
lm(Y ~ I(X^2) + X)
#Ex 3.3 Perform model selection of the previous models using AIC and BIC
exp_lm_AIC <- sapply(exp_lm_list, function(x) AIC(x))
exp_lm_AIC
exp_lm_BIC <- sapply(exp_lm_list, function(x) BIC(x))
exp_lm_df <- data.frame("Polynomial" = 1:5, "AIC" = exp_lm_AIC, "BIC" = exp_lm_BIC)
min(exp_lm_BIC) # x^3 gives the best model
exp_lm_BIC
exp_lm_AIC
lm(Y ~ X)
lm(Y ~ I(X^0) + X)
summary(lm(Y ~ I(X^0) + X))
summary(lm(Y ~ X))
lm(Y ~ I(X^1) + X)
lm(Y ~ X)
lm(Y ~ I(X^2) + X)
lm(Y ~ I(X^1) + X)
lm(Y ~ I(X^2) + X)
lm(Y ~ I(X^3) + X)
#Ex 3.2 Fit a simple linear model E(Y jX = x) = beta_0 +beta_1x, and polynomial regression models up to degree 5.
exp_lm_list <- lapply(1:5, function(i) lm(Y ~ I(X^i) + X)) #when X^1 and X are both present, only X^1 is used
exp_lm_list
#Ex 3.3 Perform model selection of the previous models using AIC and BIC
exp_lm_AIC <- sapply(exp_lm_list, function(x) AIC(x))
exp_lm_BIC <- sapply(exp_lm_list, function(x) BIC(x))
exp_lm_AIC
exp_lm_BIC
exp_lm_df <- data.frame("Polynomial" = 1:5, "AIC" = exp_lm_AIC, "BIC" = exp_lm_BIC)
min(exp_lm_BIC) # x^3 gives the best model
plot(1, type = "n", xlim=c(-0.7, 0.7), ylim = c(-4.5, 4.5), xlab = "X", ylab= "Y")
for(i in 1:5){
points(X, residuals(exp_lm_list[[i]]), col = i+1)
curve(x^i +2*x, add = TRUE, col = i+1)
}
legend("topleft", c("X^1", "X^2", "X^3", "X^4", "X^5"), col = c(2:6), lty = 1, cex = 0.75)
nls(formula = Y ~ 2*X + exp(-3* X), data = df_XY)
nls(Y ~ 2*X + exp(-3* X))
nls(Y ~ 2*X)
nls(Y ~ X)
nls(Y ~ b0 * X)
nls(Y ~ 2 * X)
nls(Y ~ b0 * X)
nls(Y ~ b0 * X _ b1)
nls(Y ~ b0 * X + b1)
nls(Y ~ b0 + b1 * X + exp(b2 * X))
nls(Y ~ b0 + b1 * X + exp(b2 * X))
?nls
nls(Y ~ b0 + b1 * X + exp(b2 * X), start = c(1,1,1))
nls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0 = 1, b1 = 1, b2 = 1))
summarynls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0 = 1, b1 = 1, b2 = 1)))
summarynls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0 = 1, b1 = 1, b2 = 1))
summary(nls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0 = 1, b1 = 1, b2 = 1)))
nls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0 = 1, b1 = 1, b2 = 1))
nls(Y ~ b0 * X + exp(b1* X))
nls(Y ~ b0 * X + exp(b1* X) + b2)
nls(Y ~ b0 * X + exp(b1* X))
nls(Y ~ b0 * X + exp(b1* X))
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ b0 * X + exp(b1* X))
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + I(exp(parameters[3]* X))
return(-sum(yvals - Y_est))
}
optim(par = c(1, 1, 1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + I(exp(parameters[3]* xvals))
return(-sum(yvals - Y_est))
}
optim(par = c(1, 1, 1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals))
return(-sum(yvals - Y_est))
}
optim(par = c(1, 1, 1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
return(-sum(yvals - Y_est))
}
optim(par = c(1, 1), fn = RSS, xvals = X, yvals = Y)
nls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0 = 1, b1 = 1, b2 = 1))
nls(Y ~ b0 + b1 * X + exp(b2 * X))
X
plot(X)
plot(X, Y)
plot(Y)
sort(X)
plot(sort(X))
plot(sort(Y))
plot(sort(X), sort(Y))
plot(X, Y)
plot(rnorm(1000))
table(X)
plot(table(X))
table(rnorm(1000))
plot(table(rnorm(1000)))
plot(rnorm(1000))
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ b0 * X + exp(b1* X))
nls(Y ~ b0 + b1 * X + exp(b2 * X))
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ b0 * X + exp(b1* X))
summary(exp_lm)
summary(nls(Y ~ b0 + b1 * X + exp(b2 * X)))
nls(Y ~ b0 + b1 * X + exp(b2 * X))
nls(Y ~ b1 * X + exp(b2 * X))
nls(Y ~ b1 * X + exp(b2 * X), start = c(10, 10))
nls(Y ~ b1 * X + exp(b2 * X), start = list(b1=10, b2=10))
nls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0= 1, b1=10, b2=10))
nls(Y ~ b0 + b1 * X + exp(b2 * X), start = list(b0= 1, b1=1, b2=1))
nls(Y ~ b1 * X + exp(b2 * X), start = list(b1=1, b2=1))
nls(Y ~ b1 * X + exp(b2 * X), start = list(b1=1, b2=-1))
nls(Y ~ b1 * X + exp(b2 * X), start = list(b1=1, b2=-2))
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ b1 * X + exp(b2 * X), start = list(b1=1, b2=-1))
summary(exp_lm)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
return(-sum(yvals - Y_est))
}
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
return(-sum(yvals - Y_est))
}
optim(par = c(1, 1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
square <- (yvals - Y_est)^2
return(-sum(square))
}
optim(par = c(1, 1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, 1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
square <- (yvals - Y_est)^2
return(-sum(square))
}
optim(par = c(1, 1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, 1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
?optim
Y-X
X
(Y-X)^2
sum((Y-X)^2)
sum((Y-(2*X+exp(-3*X))^2))
sum((Y-(1*X+exp(-1*X))^2))
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals) + parameters[3]
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1, 1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
LogLikelihood <- function(parameters, xvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
return(-sum(log(Y_est)))
}
optim(par = c(1, -1), fn = LogLikelihood, xvals = X)
LogLikelihood <- function(parameters, xvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
return(sum(log(Y_est)))
}
optim(par = c(1, -1), fn = LogLikelihood, xvals = X)
LogLikelihood <- function(parameters, xvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
return(-sum(log(Y_est)))
}
optim(par = c(1, -1), fn = LogLikelihood, xvals = X)
sum(log(2*X+exp(-3*X)))
sum(log(1*X+exp(-1*X)))
sum(log(3*X+exp(-4*X)))
optim(par = c(1, -1), fn = LogLikelihood, xvals = X)
LogLikelihood <- function(parameters, xvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
return(sum(log(Y_est)))
}
optim(par = c(1, -1), fn = LogLikelihood, xvals = X)
LogLikelihood <- function(parameters, xvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
return(-sum(log(Y_est)))
}
optim(par = c(1, -1), fn = LogLikelihood, xvals = X)
summary(exp_lm)
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
wine_model_forward_step
#Ex 4
wines <- read.csv("winequality-red.csv", sep =";")
#Ex 4.1 Fit a linear regression model using all the regressors. Use the function summary, based on the results of the t-test which are the important regressors?
wine_model <- lm(quality ~ ., data = wines)
summary(wine_model)
wine_model_forward <- lm(quality ~ 1, data=wines)
wine_model_forward
fitAll <- lm(quality ~., data=wines)
wine_model_forward_step <- step(wine_model_forward, scope= formula(fitAll), direction = "forward")
plot(XY$X, XY$Y)
abline(a= 3, b = 12, col="red")
modelXY(GenXY(a = 3,b = 2)) #p value is 0.18 so we do not reject H_0
modelXY(GenXY(a = 3,b = 2, n = 1000)) #p value is very small so we reject H_0
modelXY(GenXY(a = 3,b = 2, sd = 100)) #p value is 0.0.948 so we do not reject H_0
#Ex 2.3 Plot the predictor variable vs the residuals and the Q-Q plot of the residuals vs the normal quantiles (qqnorm and qqline functions), comment the plots.
plot(X, residuals(poly_lm_simple))
X <- rnorm(50, mean = 0, sd = 1)
Y <- sapply(X, function(x) rnorm(1, mean= x^2 - x + 1, sd = 2))
#Ex 2.3 Plot the predictor variable vs the residuals and the Q-Q plot of the residuals vs the normal quantiles (qqnorm and qqline functions), comment the plots.
plot(X, residuals(poly_lm_simple))
#Residuals seem random so not correlation with x. This means our distribution is linear
qqnorm(residuals(poly_lm_simple))
qqline(residuals(poly_lm_simple), col = "red")
curve(x^2 -x + 1, add = TRUE, col = "red")
abline(poly_lm_simple, col = "blue")
poly_lm_advanced <- lm(Y ~ I(X^2) + X)
abline(poly_lm_advanced, col = "green")
legend("topright", c("True function", "Simple regression", "Advanced regression"), col = c("red", "blue", "green"), lty = 1)
plot(2:10, poly_lm_AIC, col = "red")
plot(2:10, poly_lm_LL, col = "blue")
poly_lm_list <- lapply(2:10, function(i) lm(Y ~ I(X^i)))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(2:10, poly_lm_AIC, col = "red")
poly_lm_list <- lapply(1:10, function(i) lm(Y ~ I(X^i)))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(1:10, poly_lm_AIC, col = "red")
plot(1:10, poly_lm_LL, col = "blue")
plot(1:10, poly_lm_AIC, col = "red")
plot(1:10, poly_lm_AIC, col = "red", type = "l")
poly_lm_list
#Ex 2.7 Try to fit now a polynomial of higher degree (e.g. 3,4,5,...). Perform also here model selection. In particular plot the AIC (or BIC) score as a function of the polynomial degree. Plot also the log-likelihood as a function of the polynomial degree. What can you observe? What is the difference between the pure log-likelihood and the AIC and BIC scores?
poly_lm_list <- lapply(1:10, function(i) lm(Y ~ I(X^i)))
plot(1:10, poly_lm_AIC, col = "red", type = "l")
poly_lm_list <- lapply(1:10, function(i) lm(Y ~ poly(X, degree = i)))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_AIC
X <- rnorm(50, mean = 0, sd = 1)
Y <- sapply(X, function(x) rnorm(1, mean= x^2 - x + 1, sd = 2))
plot(X, Y)
#Ex 2.7 Try to fit now a polynomial of higher degree (e.g. 3,4,5,...). Perform also here model selection. In particular plot the AIC (or BIC) score as a function of the polynomial degree. Plot also the log-likelihood as a function of the polynomial degree. What can you observe? What is the difference between the pure log-likelihood and the AIC and BIC scores?
poly_lm_list <- lapply(1:10, function(i) lm(Y ~ I(X^i)))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_AIC <- sapply(poly_lm_list, function(x) AIC(x))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(1:10, poly_lm_AIC, col = "red", type = "l")
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(1:10, poly_lm_LL, col = "blue", type = "l")
poly_lm_list <- lapply(1:10, function(i) lm(Y ~ poly(X, degree = i)))
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(1:10, poly_lm_LL, col = "blue", type = "l")
plot(1:10, poly_lm_AIC, col = "red", type = "l")
plot(1:10, poly_lm_LL, col = "blue", type = "l")
poly_lm_LL <- sapply(poly_lm_list, function(x) logLik(x))
plot(1:10, poly_lm_LL, col = "blue", type = "l")
#Ex 2.7 Try to fit now a polynomial of higher degree (e.g. 3,4,5,...). Perform also here model selection. In particular plot the AIC (or BIC) score as a function of the polynomial degree. Plot also the log-likelihood as a function of the polynomial degree. What can you observe? What is the difference between the pure log-likelihood and the AIC and BIC scores?
poly_lm_list <- lapply(1:10, function(i) lm(Y ~ I(X^i)))
plot(1:10, poly_lm_LL, col = "blue", type = "l")
plot(1:10, poly_lm_AIC, col = "red", type = "l")
plot(1:10, poly_lm_LL, col = "blue", type = "l")
poly_lm_LL <- sapply(poly_lm_list, function(x) -logLik(x))
plot(1:10, poly_lm_LL, col = "blue", type = "l")
poly_lm_LL <- sapply(poly_lm_list, function(x) logLik(x))
plot(1:10, poly_lm_LL, col = "blue", type = "l")
X <- rnorm(50, mean=0, sd = 0.25)
Y <- sapply(X, function(x) rnorm(1, mean= exp(-3*x) + 2*x, sd = 2))
plot(rnorm(1000))
plot(X, Y)
X <- rnorm(50, mean=0, sd = 0.25)
Y <- sapply(X, function(x) rnorm(1, mean= exp(-3*x) + 2*x, sd = 2))
plot(X, Y)
Y <- sapply(X, function(x) rnorm(1, mean= exp(-3*x) + 2*x, sd = sqrt(2)))
plot(X, Y)
X <- rnorm(50, mean=0, sd = 0.5)
Y <- sapply(X, function(x) rnorm(1, mean= exp(-3*x) + 2*x, sd = sqrt(2)))
plot(X, Y)
#Ex 3.2 Fit a simple linear model E(Y jX = x) = beta_0 +beta_1x, and polynomial regression models up to degree 5.
exp_lm_list <- lapply(1:5, function(i) lm(Y ~ I(X^i))) #when X^1 and X are both present, only X^1 is used
#Ex 3.3 Perform model selection of the previous models using AIC and BIC
exp_lm_AIC <- sapply(exp_lm_list, function(x) AIC(x))
exp_lm_BIC <- sapply(exp_lm_list, function(x) BIC(x))
exp_lm_df <- data.frame("Polynomial" = 1:5, "AIC" = exp_lm_AIC, "BIC" = exp_lm_BIC)
min(exp_lm_BIC) # x^1 gives the best model
exp_lm_df
exp_lm_AIC
exp_lm_BIC
for(i in 1:5){
points(X, residuals(exp_lm_list[[i]]), col = i+1)
curve(x^i +2*x, add = TRUE, col = i+1)
}
legend("topleft", c("X^1", "X^2", "X^3", "X^4", "X^5"), col = c(2:6), lty = 1, cex = 0.75)
#The higher the power of X, the higher the residuals, so X^1 has the best fit
plot(1, type = "n", xlim=c(-0.7, 0.7), ylim = c(-4.5, 4.5), xlab = "X", ylab= "Y")
for(i in 1:5){
points(X, residuals(exp_lm_list[[i]]), col = i+1)
curve(x^i, add = TRUE, col = i+1)
}
legend("topleft", c("X^1", "X^2", "X^3", "X^4", "X^5"), col = c(2:6), lty = 1, cex = 0.75)
#The higher the power of X, the higher the residuals, so X^1 has the best fit
plot(1, type = "n", xlim=c(-0.7, 0.7), ylim = c(-4.5, 4.5), xlab = "X", ylab= "Y")
for(i in 1:5){
points(X, residuals(exp_lm_list[[i]]), col = i+1)
curve(x^i, add = TRUE, col = i+1)
}
legend("topleft", c("X^1", "X^2", "X^3", "X^4", "X^5"), col = c(2:6), lty = 1, cex = 0.75)
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ b1 * X + exp(b2 * X))
summary(exp_lm)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ b0 + b1 * X + exp(b2 * X))
summary(exp_lm)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
optim(par = c(1, -1, 1), fn = RSS, xvals = X, yvals = Y)
#Ex 3.5 Now fit the true model E(Y jX = x) = Beta_0 +Beta_1x+exp(Beta_2x) with Gaussian noise.
exp_lm <- nls(Y ~ b1 * X + exp(b2 * X))
summary(exp_lm)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1, 1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] * xvals + exp(parameters[2]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, -1), fn = RSS, xvals = X, yvals = Y)
RSS <- function(parameters, xvals, yvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
square <- (yvals - Y_est)^2
return(sum(square))
}
optim(par = c(1, 1, -1), fn = RSS, xvals = X, yvals = Y)
LogLikelihood <- function(parameters, xvals){
Y_est <- parameters[1] + parameters[2] * xvals + exp(parameters[3]* xvals)
return(-sum(log(Y_est)))
}
optim(par = c(1, 1, -1), fn = LogLikelihood, xvals = X)
